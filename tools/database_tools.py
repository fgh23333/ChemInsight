import os
import re
import json
import pymysql
from loguru import logger
from dotenv import load_dotenv
from datetime import datetime
from typing import Optional, List

load_dotenv()

# Database connection details
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_USER = os.getenv("DB_USER", "root")
DB_PASSWORD = os.getenv("DB_PASSWORD", "123456")

def get_db_connection(db_name=None):
    try:
        return pymysql.connect(
            host=DB_HOST,
            port=DB_PORT,
            user=DB_USER,
            password=DB_PASSWORD,
            database=db_name,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )
    except Exception as e:
        logger.error(f"Êï∞ÊçÆÂ∫ìËøûÊé•Â§±Ë¥•: {e}")
        return None

def list_databases() -> str:
    """ËøûÊé•Âà∞MySQLÊúçÂä°Âô®Âπ∂ÂàóÂá∫ÊâÄÊúâÊï∞ÊçÆÂ∫ìÁöÑÂêçÁß∞„ÄÇÂΩì‰∏çÁ°ÆÂÆöÊúâÂì™‰∫õÊï∞ÊçÆÂ∫ìÂèØÁî®Êó∂Ë∞ÉÁî®„ÄÇ"""
    logger.info("--- üõ†Ô∏è ÊâßË°åÂ∑•ÂÖ∑: list_databases ---")
    conn = get_db_connection()
    if conn is None:
        return "Êó†Ê≥ïËøûÊé•Âà∞Êï∞ÊçÆÂ∫ìÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆ„ÄÇ"
    try:
        with conn.cursor() as cursor:
            cursor.execute("SHOW DATABASES")
            databases = [db['Database'] for db in cursor.fetchall()]
            user_databases = [db for db in databases if db not in ['information_schema', 'mysql', 'performance_schema', 'sys']]
            result = json.dumps(user_databases, ensure_ascii=False, indent=2)
            logger.info(f"Â∑•ÂÖ∑ËæìÂá∫: {result}")
            return result
    except Exception as e:
        logger.error(f"ÂàóÂá∫Êï∞ÊçÆÂ∫ìÂ§±Ë¥•: {e}")
        return f"ÂàóÂá∫Êï∞ÊçÆÂ∫ìÂ§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ: {e}"
    finally:
        conn.close()

def get_schema_of_database(db_name: str) -> str:
    """
    Ëé∑ÂèñÊåáÂÆöÊï∞ÊçÆÂ∫ìÁöÑÂÆåÊï¥Ë°®ÁªìÊûÑÔºàÂåÖÊã¨Ë°®Âêç„ÄÅÂ≠óÊÆµÂêç„ÄÅÂ≠óÊÆµÁ±ªÂûã„ÄÅ‰∏ªÈîÆ„ÄÅÊòØÂê¶ÂèØÁ©∫„ÄÅÈªòËÆ§ÂÄºÂíåÊ≥®ÈáäÔºâ„ÄÇ
    ËøôÊòØÁêÜËß£Êï∞ÊçÆÂ∫ìÁªìÊûÑÁöÑÂÖ≥ÈîÆÂ∑•ÂÖ∑ÔºåÂú®ÊûÑÂª∫SQLÊü•ËØ¢ÂâçÂøÖÈ°ªË∞ÉÁî®„ÄÇ
    """
    logger.info(f"--- üõ†Ô∏è ÊâßË°åÂ∑•ÂÖ∑: get_schema_of_database (db_name='{db_name}') ---")
    if not re.match(r'^[a-zA-Z0-9_-]+$', db_name):
        return "Êó†ÊïàÁöÑÊï∞ÊçÆÂ∫ìÂêçÁß∞„ÄÇ"
    conn = get_db_connection(db_name)
    if conn is None:
        return "Êó†Ê≥ïËøûÊé•Âà∞Êï∞ÊçÆÂ∫ìÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆÊàñÊï∞ÊçÆÂ∫ìÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ„ÄÇ"
    try:
        with conn.cursor() as cursor:
            cursor.execute("SHOW TABLES")
            tables = [table[f'Tables_in_{db_name}'] for table in cursor.fetchall()]
            schema_info = {}
            for table_name in tables:
                cursor.execute(f"SHOW FULL COLUMNS FROM `{table_name}`")
                columns = cursor.fetchall()
                table_comment_query = f"SELECT TABLE_COMMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = '{db_name}' AND TABLE_NAME = '{table_name}'"
                cursor.execute(table_comment_query)
                table_comment_result = cursor.fetchone()
                table_comment = table_comment_result['TABLE_COMMENT'] if table_comment_result else ""
                schema_info[table_name] = {
                    "columns": [{"field": col['Field'], "type": col['Type'], "null": col['Null'], "key": col['Key'], "default": col['Default'], "extra": col['Extra'], "comment": col['Comment']} for col in columns],
                    "table_comment": table_comment
                }
            result = json.dumps(schema_info, ensure_ascii=False, indent=2)
            logger.info(f"Â∑•ÂÖ∑ËæìÂá∫: {result}")
            return result
    except Exception as e:
        logger.error(f"Ëé∑ÂèñÊï∞ÊçÆÂ∫ì '{db_name}' ÁªìÊûÑÂ§±Ë¥•: {e}")
        return f"Ëé∑ÂèñÊï∞ÊçÆÂ∫ì '{db_name}' ÁªìÊûÑÂ§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ: {e}"
    finally:
        conn.close()

def run_readonly_query_in_database(db_name: str, query: str) -> str:
    """
    Âú®ÊåáÂÆöÁöÑÊï∞ÊçÆÂ∫ì‰∏≠ÊâßË°åÂè™ËØªSQLÊü•ËØ¢„ÄÇ
    ÂÖÅËÆ∏ÁöÑÊü•ËØ¢Á±ªÂûãÂåÖÊã¨ 'SELECT', 'SHOW', 'DESCRIBE', 'EXPLAIN'„ÄÇ
    Á¶ÅÊ≠¢ÊâßË°å‰ªª‰ΩïÂèØËÉΩ‰øÆÊîπÊï∞ÊçÆÂ∫ìÁä∂ÊÄÅÁöÑÂÜôÊìç‰ΩúÔºàÂ¶Ç INSERT, UPDATE, DELETE, CREATE, DROP, ALTERÔºâ„ÄÇ
    ËøîÂõûÊü•ËØ¢ÁªìÊûúÁöÑJSONÂ≠óÁ¨¶‰∏≤„ÄÇ
    """
    logger.info(f"--- üõ†Ô∏è ÊâßË°åÂ∑•ÂÖ∑: run_readonly_query_in_database (db_name='{db_name}', query='{query}') ---")
    if not re.match(r'^[a-zA-Z0-9_-]+$', db_name):
        return "Êó†ÊïàÁöÑÊï∞ÊçÆÂ∫ìÂêçÁß∞„ÄÇ"
    query_upper = query.strip().upper()
    disallowed_keywords = ["INSERT", "UPDATE", "DELETE", "REPLACE", "CREATE", "DROP", "ALTER", "TRUNCATE", "GRANT", "REVOKE", "LOCK", "UNLOCK"]
    if any(re.search(r'\b' + keyword + r'\b', query_upper) for keyword in disallowed_keywords):
        return "Ê£ÄÊµãÂà∞ÊΩúÂú®ÁöÑÂÜôÊìç‰ΩúÔºåÂ∑≤Á¶ÅÊ≠¢ÊâßË°å„ÄÇÂè™ÂÖÅËÆ∏ÊâßË°å‰∏ç‰øÆÊîπÊï∞ÊçÆÂ∫ìÁöÑÊü•ËØ¢„ÄÇ"
    conn = get_db_connection(db_name)
    if conn is None:
        return "Êó†Ê≥ïËøûÊé•Âà∞Êï∞ÊçÆÂ∫ìÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆÊàñÊï∞ÊçÆÂ∫ìÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ„ÄÇ"
    try:
        with conn.cursor() as cursor:
            cursor.execute(query)
            result = cursor.fetchall()
            result_json = json.dumps(result, ensure_ascii=False, indent=2)
            logger.info(f"Â∑•ÂÖ∑ËæìÂá∫: {result_json}")
            return result_json
    except Exception as e:
        logger.error(f"ÊâßË°åÊü•ËØ¢Â§±Ë¥•: {e}")
        return f"ÊâßË°åÊü•ËØ¢Â§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ: {e}"
    finally:
        conn.close()

def list_tables_in_database(db_name: str) -> str:
    """
    „ÄêÂêéÂ§áÂ∑•ÂÖ∑„ÄëÂΩìget_schema_of_databaseÂ∑•ÂÖ∑Â§±Ë¥•Êó∂ÔºåÁî®‰∫éÂàóÂá∫ÊåáÂÆöÊï∞ÊçÆÂ∫ì‰∏≠ÁöÑÊâÄÊúâË°®Âêç„ÄÇ
    """
    logger.info(f"--- üõ†Ô∏è ÊâßË°åÂêéÂ§áÂ∑•ÂÖ∑: list_tables_in_database (db_name='{db_name}') ---")
    if not re.match(r'^[a-zA-Z0-9_-]+$', db_name):
        return "Êó†ÊïàÁöÑÊï∞ÊçÆÂ∫ìÂêçÁß∞„ÄÇ"
    conn = get_db_connection(db_name)
    if conn is None:
        return "Êó†Ê≥ïËøûÊé•Âà∞Êï∞ÊçÆÂ∫ìÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆÊàñÊï∞ÊçÆÂ∫ìÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ„ÄÇ"
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"USE `{db_name}`")
            cursor.execute("SHOW TABLES")
            tables = [table[f'Tables_in_{db_name}'] for table in cursor.fetchall()]
            result = json.dumps(tables, ensure_ascii=False, indent=2)
            logger.info(f"Â∑•ÂÖ∑ËæìÂá∫: {result}")
            return result
    except Exception as e:
        logger.error(f"ÂàóÂá∫Êï∞ÊçÆÂ∫ì '{db_name}' ‰∏≠ÁöÑË°®Â§±Ë¥•: {e}")
        return f"ÂàóÂá∫Êï∞ÊçÆÂ∫ì '{db_name}' ‰∏≠ÁöÑË°®Â§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ: {e}"
    finally:
        conn.close()

def describe_table_in_database(db_name: str, table_name: str) -> str:
    """
    „ÄêÂêéÂ§áÂ∑•ÂÖ∑„ÄëÂΩìget_schema_of_databaseÂ∑•ÂÖ∑Â§±Ë¥•Êó∂ÔºåÁî®‰∫éËé∑ÂèñÊåáÂÆöÊï∞ÊçÆÂ∫ì‰∏≠Âçï‰∏™Ë°®ÁöÑËØ¶ÁªÜÁªìÊûÑ„ÄÇ
    """
    logger.info(f"--- üõ†Ô∏è ÊâßË°åÂêéÂ§áÂ∑•ÂÖ∑: describe_table_in_database (db_name='{db_name}', table_name='{table_name}') ---")
    if not re.match(r'^[a-zA-Z0-9_-]+$', db_name) or not re.match(r'^[a-zA-Z0-9_-]+$', table_name):
        return "Êó†ÊïàÁöÑÊï∞ÊçÆÂ∫ìÊàñË°®ÂêçÁß∞„ÄÇ"
    conn = get_db_connection(db_name)
    if conn is None:
        return "Êó†Ê≥ïËøûÊé•Âà∞Êï∞ÊçÆÂ∫ìÔºåËØ∑Ê£ÄÊü•ÈÖçÁΩÆÊàñÊï∞ÊçÆÂ∫ìÂêçÁß∞ÊòØÂê¶Ê≠£Á°Æ„ÄÇ"
    try:
        with conn.cursor() as cursor:
            cursor.execute(f"USE `{db_name}`")
            cursor.execute(f"SHOW FULL COLUMNS FROM `{table_name}`")
            columns = cursor.fetchall()
            table_comment_query = f"SELECT TABLE_COMMENT FROM information_schema.TABLES WHERE TABLE_SCHEMA = '{db_name}' AND TABLE_NAME = '{table_name}'"
            cursor.execute(table_comment_query)
            table_comment_result = cursor.fetchone()
            table_comment = table_comment_result['TABLE_COMMENT'] if table_comment_result else ""
            table_info = {
                "table_name": table_name,
                "table_comment": table_comment,
                "columns": [{"field": col['Field'], "type": col['Type'], "null": col['Null'], "key": col['Key'], "default": col['Default'], "extra": col['Extra'], "comment": col['Comment']} for col in columns]
            }
            result = json.dumps(table_info, ensure_ascii=False, indent=2)
            logger.info(f"Â∑•ÂÖ∑ËæìÂá∫: {result}")
            return result
    except Exception as e:
        logger.error(f"Ëé∑ÂèñË°® '{table_name}' ÁªìÊûÑÂ§±Ë¥•: {e}")
        return f"Ëé∑ÂèñË°® '{table_name}' ÁªìÊûÑÂ§±Ë¥•ÔºåÈîôËØØ‰ø°ÊÅØ: {e}"
    finally:
        conn.close()

def build_workstation_analysis_query(
    start_time: str,
    end_time: str,
    analysis_type: str,
    aggregations: Optional[List[str]] = None,
    interval_minutes: Optional[int] = None,
    station_codes: Optional[List[str]] = None,
    order_by: Optional[str] = None,
    limit: Optional[int] = None
) -> str:
    """
    ÊûÑÂª∫‰∏Ä‰∏™Áªü‰∏Ä„ÄÅÂ§öÂäüËÉΩ‰∏îÁ≤æÁ°ÆÁöÑSQLÊü•ËØ¢ÔºåÁî®‰∫éÂàÜÊûêÂ∑•‰ΩúÁ´ôÁöÑÂç†Áî®ÊÉÖÂÜµ„ÄÇ
    Ê≠§ÁâàÊú¨ÂÜÖÁΩÆ‰∫ÜÂÖàËøõÁöÑ‚ÄúÂå∫Èó¥ÂêàÂπ∂‚ÄùÈÄªËæëÔºåÂèØ‰ª•Ê≠£Á°ÆÂ§ÑÁêÜÈáçÂè†ÁöÑÊó∂Èó¥ÊÆµÔºåÂπ∂‰øÆÂ§ç‰∫ÜJSONÂ∫èÂàóÂåñÈóÆÈ¢ò„ÄÇ

    ÂèÇÊï∞:
    - start_time (str): ÂàÜÊûêÁöÑÂºÄÂßãÊó∂Èó¥ÔºåÊ†ºÂºè 'YYYY-MM-DD HH:MM:SS'„ÄÇ
    - end_time (str): ÂàÜÊûêÁöÑÁªìÊùüÊó∂Èó¥ÔºåÊ†ºÂºè 'YYYY-MM-DD HH:MM:SS'„ÄÇ
    - analysis_type (str): ÂàÜÊûêÁ±ªÂûãÔºåÂÜ≥ÂÆöÊü•ËØ¢ÁöÑÊ®°Âºè„ÄÇÊúâÊïàÈÄâÈ°π:
        - 'summary_stats': (ÊúÄÂ∏∏Áî®)ÁîüÊàêÂáÜÁ°ÆÁöÑËÅöÂêàÁªüËÆ°Êï∞ÊçÆÔºàÂ¶ÇÊÄªÊó∂Èïø„ÄÅÊ¨°Êï∞Ôºâ„ÄÇÂü∫‰∫éÂêàÂπ∂ÂêéÁöÑÊó∂Èó¥ÊÆµËÆ°ÁÆó„ÄÇ
        - 'time_series': ÁîüÊàêÂç†Áî®ÁéáÁöÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆ„ÄÇÂü∫‰∫éÂêàÂπ∂ÂêéÁöÑÊó∂Èó¥ÊÆµËÆ°ÁÆóÔºåÁªìÊûúÁ≤æÁ°Æ„ÄÇ
        - 'raw_periods': Ëé∑ÂèñÂéüÂßãÁöÑÂç†Áî®Êó∂ÊÆµ„ÄÇÊ≠§Ê®°Âºè‰∏çËøõË°åÂêàÂπ∂ÔºåÁî®‰∫éÊü•ÁúãÂéüÂßãÊï∞ÊçÆ„ÄÇ
    - aggregations (Optional[List[str]]): ÂΩì analysis_type='summary_stats' Êó∂‰ΩøÁî®„ÄÇ
        ÊúâÊïàÈÄâÈ°π: ['count', 'total_duration', 'avg_duration', 'max_duration']„ÄÇ
    - interval_minutes (Optional[int]): ÂΩì analysis_type='time_series' Êó∂**ÂøÖÈ°ª**Êèê‰æõ„ÄÇÂÆö‰πâÊó∂Èó¥ÂàáÁâáÁöÑÂàÜÈíüÊï∞„ÄÇ
    - station_codes (Optional[List[str]]): ÂèØÈÄâÔºåÁî®‰∫éÁ≠õÈÄâ‰∏Ä‰∏™ÊàñÂ§ö‰∏™ÁâπÂÆöÂ∑•‰ΩúÁ´ô„ÄÇ
    - order_by (Optional[str]): ÂèØÈÄâÔºåÁî®‰∫éÊéíÂ∫è„ÄÇÊ†ºÂºè: 'ÊåáÊ†áÂêç_asc' Êàñ 'ÊåáÊ†áÂêç_desc'„ÄÇ
        ‰æãÂ¶Ç: 'total_duration_desc'„ÄÇ
    - limit (Optional[int]): ÂèØÈÄâÔºåÈôêÂà∂ËøîÂõûÁöÑÁªìÊûúË°åÊï∞„ÄÇ
    """
    logger.info(
        f"--- üõ†Ô∏è ÊâßË°åÈ´òÁ∫ßÂàÜÊûêÂ∑•ÂÖ∑ (analysis_type='{analysis_type}', start_time='{start_time}', "
        f"end_time='{end_time}', aggregations={aggregations}, interval_minutes={interval_minutes}, "
        f"station_codes={station_codes}, order_by={order_by}, limit={limit}) ---"
    )
    try:
        datetime.strptime(start_time, '%Y-%m-%d %H:%M:%S')
        datetime.strptime(end_time, '%Y-%m-%d %H:%M:%S')
    except ValueError:
        return "Êó†ÊïàÁöÑÊó∂Èó¥Ê†ºÂºè„ÄÇËØ∑‰ΩøÁî® 'YYYY-MM-DD HH:MM:SS' Ê†ºÂºè„ÄÇ"
    if analysis_type not in ['summary_stats', 'time_series', 'raw_periods']:
        return f"ÈîôËØØ: Êó†ÊïàÁöÑ 'analysis_type'„ÄÇËØ∑‰ªé 'summary_stats', 'time_series', 'raw_periods' ‰∏≠ÈÄâÊã©„ÄÇ"
    
    station_filter_clause = ""
    if station_codes:
        safe_stations = [f"'{station.replace('\'', '')}'" for station in station_codes]
        station_filter_clause = f"AND t.station_code IN ({','.join(safe_stations)})"

    final_query = ""
    if analysis_type == 'summary_stats':
        if not aggregations:
            return "ÈîôËØØ: analysis_type='summary_stats' ÈúÄË¶ÅÊèê‰æõ 'aggregations' ÂèÇÊï∞„ÄÇ"
        merging_logic_ctes = f"""
WITH
  raw_periods AS (
    SELECT t.station_code, l.finish_time AS arrival_time, MIN(d.execute_time) AS departure_time
    FROM phoenix_rms.job_history l JOIN phoenix_rms.job_history d ON l.upstream_order_no = d.upstream_order_no AND d.job_template_code = 'DO_MOVE' AND d.execute_time > l.finish_time
    JOIN phoenix_rss.transport_order_qp_job t ON l.upstream_order_no = t.upstream_order_no
    WHERE l.job_template_code = 'QL_MoveLiftUp_Move' AND l.finish_time BETWEEN '{start_time}' AND '{end_time}' AND t.station_code IS NOT NULL {station_filter_clause}
    GROUP BY t.station_code, l.upstream_order_no, l.finish_time
  ),
  period_islands AS (
    SELECT station_code, arrival_time, departure_time,
      CASE WHEN arrival_time > MAX(departure_time) OVER (PARTITION BY station_code ORDER BY arrival_time, departure_time ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) THEN 1 ELSE 0 END AS is_island_start
    FROM raw_periods
  ),
  period_with_island_id AS (
    SELECT station_code, arrival_time, departure_time, SUM(is_island_start) OVER (PARTITION BY station_code ORDER BY arrival_time, departure_time) AS island_id
    FROM period_islands
  ),
  merged_periods AS (
    SELECT station_code, MIN(arrival_time) AS start_time, MAX(departure_time) AS end_time
    FROM period_with_island_id GROUP BY station_code, island_id
  ),
  final_durations AS (
    SELECT station_code, TIMESTAMPDIFF(SECOND, start_time, end_time) AS duration_seconds
    FROM merged_periods
  )
"""
        aggregation_map = {
            'count': "COUNT(*) AS merged_periods_count",
            'total_duration': "SUM(duration_seconds) AS total_duration_seconds",
            'avg_duration': "AVG(duration_seconds) AS avg_duration_seconds",
            'max_duration': "MAX(duration_seconds) AS max_duration_seconds"
        }
        select_clauses = ["station_code"] + [aggregation_map[agg] for agg in aggregations if agg in aggregation_map]
        order_by_statement = f"ORDER BY {order_by.replace('_desc', ' DESC').replace('_asc', ' ASC')}" if order_by else ""
        limit_statement = f"LIMIT {limit}" if limit else ""
        final_query = f"""{merging_logic_ctes}
SELECT {', '.join(select_clauses)}
FROM final_durations
GROUP BY station_code
{order_by_statement} {limit_statement};
"""
    elif analysis_type == 'time_series':
        if not interval_minutes or interval_minutes <= 0:
            return "ÈîôËØØ: analysis_type='time_series' ÈúÄË¶ÅÊèê‰æõÊ≠£Êï¥Êï∞ 'interval_minutes' ÂèÇÊï∞„ÄÇ"
        merging_logic_ctes = f"""
WITH RECURSIVE
  raw_periods AS (
    SELECT t.station_code, l.finish_time AS arrival_time, MIN(d.execute_time) AS departure_time
    FROM phoenix_rms.job_history l JOIN phoenix_rms.job_history d ON l.upstream_order_no = d.upstream_order_no AND d.job_template_code = 'DO_MOVE' AND d.execute_time > l.finish_time
    JOIN phoenix_rss.transport_order_qp_job t ON l.upstream_order_no = t.upstream_order_no
    WHERE l.job_template_code = 'QL_MoveLiftUp_Move' AND l.finish_time BETWEEN '{start_time}' AND '{end_time}' AND t.station_code IS NOT NULL {station_filter_clause}
    GROUP BY t.station_code, l.upstream_order_no, l.finish_time
  ),
  period_islands AS (
    SELECT station_code, arrival_time, departure_time, CASE WHEN arrival_time > MAX(departure_time) OVER (PARTITION BY station_code ORDER BY arrival_time, departure_time ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) THEN 1 ELSE 0 END AS is_island_start
    FROM raw_periods
  ),
  period_with_island_id AS (
    SELECT station_code, arrival_time, departure_time, SUM(is_island_start) OVER (PARTITION BY station_code ORDER BY arrival_time, departure_time) AS island_id
    FROM period_islands
  ),
  merged_periods AS (
    SELECT station_code, MIN(arrival_time) AS start_time, MAX(departure_time) AS end_time
    FROM period_with_island_id GROUP BY station_code, island_id
  ),
  time_slices AS (
    SELECT '{start_time}' AS slice_start, TIMESTAMPADD(MINUTE, {interval_minutes}, '{start_time}') AS slice_end
    UNION ALL
    SELECT slice_end, TIMESTAMPADD(MINUTE, {interval_minutes}, slice_end) FROM time_slices WHERE slice_end < '{end_time}'
  )
"""
        final_query = f"""{merging_logic_ctes}
SELECT
  ts.slice_start,
  CAST(
    SUM(
      TIMESTAMPDIFF(
        SECOND,
        GREATEST(mp.start_time, ts.slice_start),
        LEAST(mp.end_time, ts.slice_end)
      )
    ) AS DOUBLE
  ) / ({interval_minutes * 60}) AS occupancy_rate
FROM time_slices ts
JOIN merged_periods mp ON mp.start_time < ts.slice_end AND mp.end_time > ts.slice_start
GROUP BY ts.slice_start
ORDER BY ts.slice_start;
"""
    elif analysis_type == 'raw_periods':
        final_query = f"""
WITH raw_periods AS (
    SELECT
        t.station_code,
        l.finish_time AS arrival_time,
        MIN(d.execute_time) AS departure_time
    FROM phoenix_rms.job_history l
    JOIN phoenix_rms.job_history d ON l.upstream_order_no = d.upstream_order_no AND d.job_template_code = 'DO_MOVE' AND d.execute_time > l.finish_time
    JOIN phoenix_rss.transport_order_qp_job t ON l.upstream_order_no = t.upstream_order_no
    WHERE
        l.job_template_code = 'QL_MoveLiftUp_Move'
        AND l.finish_time BETWEEN '{start_time}' AND '{end_time}'
        AND t.station_code IS NOT NULL
        {station_filter_clause}
    GROUP BY t.station_code, l.upstream_order_no, l.finish_time
)
SELECT
    station_code,
    JSON_ARRAYAGG(JSON_ARRAY(arrival_time, departure_time)) AS occupancy_periods
FROM raw_periods
GROUP BY station_code;
"""
        logger.info(f"Â∑•ÂÖ∑ËæìÂá∫ (ÁîüÊàêÁöÑÊúÄÁªàSQLÊü•ËØ¢): \n{final_query}")
        return final_query
